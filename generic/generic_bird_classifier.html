<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Generic - Hyperspace by HTML5 UP</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="index.html" class="title">Data Frontier</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="generic/" class="active">Report</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Birdsong Audio Classification using Neural Networks and Machine Learning</h1>
							<h3 class="tech-stack">Technologies Used: Python | TensorFlow | Scikit | Deep Learning | Machine Learning</h3>
							<h2>Introduction</h2>
			<p>The project's primary goal was to develop a bird species classifier capable of accurately identifying species from audio recordings. The challenge was to navigate the complexities of processing and analyzing short, individual bird call recordings against a backdrop of significant class imbalance and
				varying audio quality.</p>

<h2>Data and Methodology</h2>
<p>We gathered the Bird clef 2023 dataset from <a href="https://www.kaggle.com/competitions/birdclef-2023/data">Kaggle</a>
The BirdCLEF 2023 dataset consisted of 16,941 audio recordings from xenocanto.org, encompassing 264 different bird species. The dataset featured primary attributes such as short recordings and identified bird species, along with secondary features including call type, location, and quality rating.In an effort to focus the project and address computing limitations, we refined our initial dataset of 10 bird species to three: Western Yellow Wagtail, Common Sandpiper, and Barn Swallow. These species were randomly selected from three different families to ensure diversity. To mitigate class imbalance, we down-sampled over-represented species at random, ensuring each species had approximately the same total duration of recordings, about 171-172 minutes. For our train/validation split, we adopted a strategy of random selection until the total training duration constituted 70% of the total duration for each species, with the remainder allocated to validation. This approach ensured a balanced representation of each species in both training and validation sets.</p>

<h2>Exploratory Data Analysis</h2>
<p> The below visualization  presents four different audio analysis plots for a Thrush Nightingale's song, including the sound wave, Root Mean Square (RMS) energy, Mel Spectrogram,
	and Mel-Frequency Cepstral Coefficients (MFCC), each providing a unique perspective on the audio characteristics.</p>
<div style="text-align:center;"><img src="images/audio_features.png" alt="audio" width="500" height="400"></div>

<p> The below hierarchical plot  showcases the "Top 10 Bird Species - Taxonomy," displaying vivid photographs of each bird along with their scientific classifications, offering a colorful and educational snapshot of avian diversity.</p>
<div style="text-align:center;"><img src="images/map.png" alt="audio" width="500" height="400"></div>

<p>The above map depicts a global distribution of various bird species, indicated by colored dots across continents, highlighting the geographical spread and habitats of the birds included in the study.</p>
<div style="text-align:center;"><img src="images/bird_map.png" alt="audio" width="500" height="400"></div>

<h2>Feature Extraction</h2>
<p>The project utilized advanced audio processing techniques to extract meaningful features from the recordings:</p>
	<ul style="margin-top: 0;">
		<li> Melspectrogram & MFCC: These visualizations capture the distribution of audio frequencies, transformed into the mel scale to align with human sound perception.</li>
		<li>Chroma:This feature summarizes the 12 different pitch classes, offering insights into the harmonic content of bird calls.</li>
		<li>RMS Energy:A measure of the signal's magnitude or "loudness" over time.</li>
		<li>Spectral Centroid:Represents the "center of mass" of the spectrum, indicating the brightness of the sound. For testing, the models processed audio clips of 8 seconds in length, with a 4-second overlap to ensure continuity and maximize the use of the audio data. The same preprocessing techniques applied during training were used to prepare approximately 3,000 test samples, representing 35% of the total samples. Normalization of features for testing was based on scalers derived from the training data to maintain consistency and accuracy in model evaluation.</li>
	</ul>


 <h2>Modeling Pipeline</h2>
 <p>A range of machine learning models was evaluated:</p>
 <ul style="margin-top: 0;">
	 <li>Baseline Models: Random guessing provided a baseline accuracy of 33%.</li>
	 <li>Traditional Algorithms: Included Random Forest, XGBoost, SVM, and Logistic Regression.</li>
	 <li>Deep Learning Models: Encompassed 1D CNN, 2D CNN, LSTM, GRU RNN, and Vision Transformers, with architectures tailored for audio classification.</li>
 </ul>


 <h2>Architecture Details </h2>
 <p>For deep learning models, architectures were specifically designed to process and learn from the complex features of audio data. For example, the 1D CNN model utilized concatenated audio features and embeddings, while the Vision Transformer applied augmented STFT spectrograms for classification.
 </p>

<h3>One Dimensonal Convolution Neural Network (1DCNN)</h3>
 <div style="text-align:center;"><img src="images/transformer.png" alt="audio" width="500" height="400"></div>

<p> The design of the 1D CNN model employed a functional API architecture, facilitating a seamless flow of data through the network layers. The architecture included:</p>
	<ul style="margin-top: 0;">
		<li>Input Layer: Combined normalized, transposed audio features with 2D learned embeddings of continents, tiled along the time axis to match the audio features' shape.</li>
		<li>Convolutional Layers: Two convolutional layers (Cov1D) were utilized, each followed by a MaxPooling layer to reduce dimensionality and capture the most salient features.</li>
		<li>Regularization: L2 regularization was applied to convolutional layers to mitigate overfitting, complemented by a 50% dropout rate before the final output layer to further enhance generalization.</li>
		<li>Optimization and Training: The Adam optimizer was selected for its efficiency in handling sparse gradients and adaptive learning rate capabilities. A callback mechanism was implemented to restore the model's best weights achieved at the 30th epoch, optimizing the training process.</li>
</ul>

<h3>Gated Recurrent Neural Network (GRU RNN)</h3>
  <p>Employing a functional API architecture allowed for a flexible and modular design approach. The architecture centered around a GRU layer with 256 units, capitalizing on the GRU's capacity for capturing temporal dependencies and sequence dynamics efficiently. To combat overfitting, the model integrated three dropout layers at critical junctures, alongside dense layers activated by ReLU, with unit counts of 128, 64, and 48. This structure was aimed at progressively refining the feature representation for accurate classification. For the final output, a softmax activation
function was utilized to facilitate multi-class classification, ensuring the model's output could be interpreted as probabilities across the different bird species.
</p>
<div style="text-align:center;"><img src="images/GRU.png" alt="audio" width="500" height="400"></div>


<h3>Vision transformer</h3>
<ul style="margin-top: 0;">
	<li>Input Spectrogram: The input to the model is an augmented STFT spectrogram, which is a visual representation of the spectrum of frequencies in the audio signal as they vary with time. This spectrogram is divided into patches (each patch could be a small time-frequency area), which are then flattened and linearly projected into a sequence of vectors (tokens). Positional embeddings are added to maintain the order of the sequence.
</li>
	<li>Transformer Blocks: The core of the ViT architecture consists of multiple Transformer blocks that process the sequence of tokens. Each block includes:</li>
	<li>Layer Normalization (Norm):  Stabilizes the learning process by normalizing the input layer.</li>
	<li>Multi-Head Self-Attention: Allows the model to weigh the importance of different parts of the input data differently. With multiple 'heads', the model can focus on different positions of the input sequence, capturing various aspects of the data.</li>
	<li>Dense Layers with GELU Activation: These fully connected layers further process the information, with the GELU (Gaussian Error Linear Unit) providing a non-linear activation function.</li>
	<li>Dropout: Used between dense layers to prevent overfitting by randomly setting a portion of the input units to 0 during training.</li>
	<li>Classifier Head: At the end of the Transformer blocks, the sequence of processed tokens is passed through additional layer normalization and dropout. The final 'classification token' (sometimes initialized and learned separately from the patch embeddings) is used to aggregate the information from the entire sequence and make a classification decision.</li>
	<li>Output Layer: The last dense layer has as many units as there are classes to predict (in this case, 3 for the bird species). It uses a softmax activation function to output a probability distribution over the classes.</li>
</ul>
<div style="text-align:center;"><img src="images/transformer.png" alt="audio" width="500" height="400"></div>

<h3>Results and Evaluation </h3>
<p>The project made significant progress from the baseline, with deep learning models showing superior performance:</p>
<li>1D CNN demonstrated an overall accuracy of 89%, with F1-scores ranging from 0.88 to 0.91 for specific bird species.</li>
<div style="text-align:center;"><img src="images/cnn_results.png" alt="audio" width="500" height="400"></div>

<li>GRU RNN achieved an overall accuracy of 87%, highlighting the effectiveness of RNN architectures in handling audio data.</li>
<div style="text-align:center;"><img src="images/gru_results.png" alt="audio" width="500" height="400"></div>

<li>Vision Transformer stood out with an overall accuracy of 94%, showcasing the advanced capability of transformer models in audio classification.</li>
<div style="text-align:center;"><img src="images/trans_results.png" alt="audio" width="500" height="400"></div>





<p style="margin-bottom: 10px;">Based on a Pearson correlation analysis, we selected several covariates that show a significant relationship with the dependent variable, Revenue. Here's a concise overview of these covariates and their relevance:</p>
<ul style="margin-top: 0;">
    <li>Vote Count: Reflects public engagement with the film.</li>
    <li>Runtime: Influences the number of possible screenings and, by extension, profitability.</li>
    <li>Popularity: A multifaceted metric gauging public and online presence.</li>
    <li>Title Length: The brevity of a title may enhance its memorability and dissemination.</li>
    <li>Release Season: Aligns movie releases with peak holiday periods for better turnout.</li>
    <li>Release Language: Addresses the potential reach of a film based on the language of release.</li>
</ul>
<p>The plot below shows the “pearson” correlation between
the dependent and all independent variables. Based on the correlation score, we chose the following covariates
that affect the dependent variable (Revenue). The color shades are used to represent correlation, spanning
from dark red to dark blue. In this scheme, dark shades of red (+1.0) indicate a strong positive correlation
between variables, whereas dark shades of blue indicate a strong negative correlation (-1.0).</p>
<div style="text-align:center;"><img src="images/corr_plot.png" alt="Linear Regression" width="500" height="400"></div>
<p>Before running the regression model, we checked the multi-collinearity between all predictors by running a
variation inflation factor to reduce unstable and unreliable estimates of the regression analysis. We observed
there is no evidence of multi-collinearity between the predictor variables.</p>
<p>We created three regression models on the 30% dataset to understand better to understand the factors
contributing to the Revenue of a movie. The first model studied the correlation between budget and Revenue.
In the second model, we gradually added covariates such as ‘Vote Count’ and ‘Run time’. Finally, in the
third model, we added more covariates such as Popularity, Movie Title Length, Release season, and Release
Language.</p>
<p>The displayed equation represents the linear regression model used in our analysis. In this model, β0 denotes the intercept, providing the starting point of the regression line. β1 is the coefficient that quantifies the influence of the budget on revenue. Additionally, Z signifies a vector containing other relevant variables chosen for the model, while γ represents their respective coefficients, indicating the weight of each covariate in the model.</p>
<div style="text-align:center;"><img src="images/eqn.png" alt="Linear Regression" width="400" height="50"></div>

<p>Additionally, we evaluated homoscedasticity to check if the variance of the errors or residuals in the model is constant across all the predictor variables using scale-location plot and Breusch Pagan test,
but the results showed heteroscedastic behavior.Since our dataset is large, we continued to build our model with robust
standard errors to handle the heteroscedastic behavior. Below is the scale-location demonstrating heteroskedastic behavior.
From below plot, we see that the red line on the plot is not horizontal and also the spread of the residuals is not equal at all the fitted values, highlighting evidences for Heteroscedasticity.

<div style="text-align:center;"><img src="images/scale_loc.png" alt="Linear Regression" width="500" height="300">
</div>
lm(log_revenue ~ log_budget + vote_count + runtime + popularity +
								log_title_length + release_date_cat + lang_cat)
<br/> <br/>


<p>We  also used Breusch-Pagan test to validate homoscedasticity. The null hypothesis(H0) of the Breusch- Pagan test is that the residuals are Homoscedastic.The alternate hypothesis(Ha) is that the residuals are heteroscedastic. If the p-value from the test is less than the chosen significance level (e.g., 0.05), we can reject the null hypothesis of homoscedasticity. From the bptest results, it is seen that the p-value (3.422e- 12) is less than 0.05 and we can reject the null hypothesis and conclude that there is strong evidence of heteroscedasticity in the linear regression model.</p>
<div style="text-align:center;"><img src="images/bp_test.png" alt="Linear Regression" width="400" height="200">
<br/>

<h2 style="text-align: left;">Statistical Tests</h2>
<h3 style="text-align: left;">T-Test</h2>
<p style="text-align: left;">Below is the results of t-test highlighting the statistical significance of each and every predictor in the selected model. Since the model is of heteroscedastic, robust standard errors are used as an effective solution to handle it.
</p>
<div style="text-align:center;"><img src="images/t_test.png" alt="Linear Regression" width="400" height="200">

<h3 style="text-align: left;">Anova Test</h2>
<p style="text-align: left;">We conducted anova test to find out best fitting model. Below results of anova test of the three models shows that model 3 is fitting better than other two models as it has significant F-statistic value along with lower residual sum of squares, low residual degrees of freedom and low p-value.
</p>
<div style="text-align:center;"><img src="images/anova.png" alt="Linear Regression" width="400" height="200">

<h3 style="text-align: left;">Wald Test</h2>
<p style="text-align: left;">We conducted wald test to estimate the significant variables (whether the variables add any value to the model or not) from a set of predictors and found out that budget, vote count, popularity, runtime, movie title length, movie language category and movie release season from model 3 influences the revenue of the movie.
</p>
<div style="text-align:center;"><img src="images/wald.png" alt="Linear Regression" width="400" height="300">


<h2 style="text-align: left;">Results</h2>
<p>The image below illustrates the results derived from applying three distinct regression models to a subset, comprising 70% of the overall dataset.From the results, We  observe that model 3 has the highest adjusted R-squared value of 0.67, indicating
that it is the best-fitting model out of the three. A<p>
<div style="text-align:center;"><img src="images/results.png" alt="Linear Regression" width="600" height="500"></div>

<p style="text-align: left;">The “Budget” variable has a strong positive relationship with revenue in all three models, with estimates
ranging from 0.91 to 0.77. All covariates, including vote count, runtime, and release season, are statistically
significant with p-values less than 0.05 (alpha level). The covariates vote count, runtime, and release season
show more statistical significance than other covariates.</p>

<p style="text-align: left;">To better understand the practical significance of the results, let’s consider a hypothetical use case with 180 minutes movie being produced with a million-dollar budget with a shorter title length of 10, the Model 3 shows that the Revenue of the movie could increase by 77.2%. Similarly, the model also shows that for every movie which is released in the English language, the revenue of the movie is expected to increase by approximately 15.6% keeping all the other covariates constant. Likewise, on average, the model predicts if a movie gets released during December/January, the revenue of the movie is expected to increase by 18.1%.<p>

<p style="text-align: left;">These results emphasize that budget is a crucial factor in determining the Revenue of a movie. It also indicates that along with the budget, the other covariates are also having significance in determining the revenue of a movie. But considering the statistical significance of popularity, movie production firms should not spend more money in promotions. Having all these into considerations, we believe that Model 3 can assist movie production companies in making informed decisions about the various factors involved in the overall planning process, which is essential for the success of the movie.<p>


<h2 style="text-align: left;">Limitations</h2>
<p style="text-align: left;">The Dataset has a chance of introducing Sampling Bias as the data about the movies is self-reported on TMDB, which is a user-generated content platform.
Thus, the dataset may represent some movies but the dataset cannot be representative of all movies that have been produced globally.
The data may also be biased toward movies that are more popular than others.
</p>

<p style="text-align: left;">The I.I.D assumption becomes questionable after closely examining the movie dataset because two movies may share the same cast & crew, and production company and have identical release dates.
We observed that the dataset has some omitted variables. For example, it does not mention any information about Motion Picture Association of America(MPAA) Ratings, which have the possibility of influencing the Revenue. Since MPAA rating are categorical, different MPAA ratings may generate different revenue. Here budget impacts the content of a movie which in-turn affects the rating of a movie, hence it is difficult to determine the direction of omitted variable bias accurately. Likewise, actors, a categorical variable, may influence both budget & Revenue of the movie where we may not be able to determine the direction of omitted variable bias.</p>

<p style="text-align: left;">
The dataset may not represent a true picture of the current movie industry/trends since it contains informa- tion on movies which has a release date before 2013. Moreover, there is no consideration of inflation when concluding the statistical analysis between covariates such as budget & Revenue. More inflation leads to more budget & revenue and the direction of the bias is away from zero.
Also, in the model, we have an outcome variable, “Vote count” on the RHS. This variable can be an outcome of the predictor “Popularity”. The reason could be because of promotions, more people can engage with the movie, which could lead to an increase in the number of votes.</p>


<h2 style="text-align: left;">Conclusion</h2>
<p style="text-align: left;">This study estimated the economic value of movie production, specifically examining the relationship between movie budget and Revenue. We also found that several covariates, such as vote_count, runtime, popularity, title length, release season, and release language, have a significant impact on movie revenue. Additionally, we identified the holiday season and language as other important categorical factors that can impact the success of a movie. We hope that this line of work will provide filmmakers with accurate tools to plan their investments and optimize their production strategies, reducing uncertainty in the film industry. Future work may explore the correlation of Revenue with additional data like MPAA ratings, cast/crew information, and sequel information on previous releases for identifying the latest trends in the movie industry.
</p>

<h2 style="text-align: left;">Code & Insights</h2>
<p style="text-align: left;">
 <a href="URL_TO_YOUR_SLIDES" target="_blank" style="color: Green; font-weight: bold;">Presentation</a> |
 <a href="https://github.com/hamsini1692/stats-lab02" target="_blank" style="color: Green; font-weight: bold;">GitHub</a>
</p>



						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
